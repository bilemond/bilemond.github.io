<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2023/01/25/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>论文解读：GAT</title>
    <url>/2023/01/29/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%EF%BC%9AGAT/</url>
    <content><![CDATA[<h2 id="论文信息">论文信息</h2>
<blockquote>
<p>论文标题：Graph Attention Networks 论文作者：Petar
Velickovic、Guillem Cucurull、Arantxa Casanova、Adriana Romero、P.
Lio’、Yoshua Bengio 论文来源：2018 ICLR 论文地址：<a
href="https://arxiv.org/abs/1710.10903">download</a> 论文代码：<a
href="https://github.com/PetarV-/GAT">download</a></p>
</blockquote>
<h2 id="introduction">INTRODUCTION</h2>
<h3 id="两种模态结构">两种模态结构</h3>
<ul>
<li>grid-like structure（网格结构）：photo</li>
<li>irregular domain：3D meshes（三维网络）, social networks, biological
networks or brain connectomes</li>
</ul>
<h3 id="目前研究">目前研究</h3>
<h4 id="图的研究">图的研究</h4>
<ul>
<li>光谱方法：学习滤波器都依赖于拉普拉斯特征基，拉普拉斯特征基依赖于图的结构。因此，<strong>在特定结构上训练的模型不能直接应用于不同结构的图</strong>。</li>
<li>非光谱方法：</li>
</ul>
<h4 id="注意力机制">注意力机制</h4>
<ul>
<li>允许处理可变大小的输入，关注输入中最相关的部分来做出决策。</li>
<li>当注意机制用于计算单个序列的表示时，通常称为自我注意或内部注意self-attention
or intra-attention</li>
</ul>
<h3 id="attention-based-architecture">attention-based architecture</h3>
<h4 id="特性">特性</h4>
<ul>
<li>操作高效，因为它可以跨<strong>节点-邻居对node- neighbor
pairs</strong>并行；</li>
<li>它可以通过给邻居指定任意的权重来适用于不同程度的图节点；</li>
<li>该模型直接适用于归纳学习问题inductive learning
problems，包括模型必须推广到完全看不见的图的任务。</li>
</ul>
<h2 id="gat-网络架构">GAT 网络架构</h2>
<h3 id="图注意力层">图注意力层</h3>
<h4 id="输入输出">输入输出</h4>
<p>层输入：<img data-src="论文解读：GAT/image-20230128090753101.png" alt="image-20230128090753101" style="zoom: 67%;" /></p>
<p>输出：<img data-src="论文解读：GAT/image-20230129132556034.png" alt="image-20230129132556034" style="zoom: 67%;" /></p>
<p>其中 <span class="math inline">\(F\)</span> 和 <span
class="math inline">\(F&#39;\)</span>表示不同的节点特征维度， <span
class="math inline">\(N\)</span>是节点个数</p>
<h4 id="注意力机制-1">注意力机制</h4>
<p>为了获得足够的表达能力将输入特征转换为更高层次的特征，Graph
attentional layer 首先根据输入的节点特征向量集，进行
self-attention处理： <span class="math display">\[
e_{i j}=a(\mathbf{W} \vec{h}_{i}, \mathbf{W} \vec{h}_{j} )
\]</span> 其中 <span class="math inline">\(a\)</span> 是一个 <span
class="math inline">\(\mathbb{R}^{F^{\prime}} \times
\mathbb{R}^{F^{\prime}} \rightarrow \mathbb{R}\)</span> 的映射；<span
class="math inline">\(W \in \mathbb{R}^{F^{\prime} \times F}\)</span>
是一个权重矩阵（被所有 <span class="math inline">\(\vec{h}_{i}\)</span>
的共享）；<span class="math inline">\(e_{ij}\)</span> 表明节点 <span
class="math inline">\(j\)</span> 的特征对节点 <span
class="math inline">\(i\)</span> 的重要性，也就是注意力系数。</p>
<p>一般来说，self-attention
会将注意力分配到图中所有的节点上，这种做法显然会丢失结构信息。为了解决这一问题，本文使用了一种
<strong>masked attention</strong> 的方式——仅将注意力分配到节点 <span
class="math inline">\(i\)</span> 的邻节点集上，即 <span
class="math inline">\(j \in \mathcal{N}_{i}\)</span>，（在本文中，节点
<span class="math inline">\(i\)</span> 也是 <span
class="math inline">\(\mathcal{N}_{i}\)</span> 的一部分）。</p>
<h4 id="归一化注意力系数">归一化注意力系数</h4>
<p>为了使注意力权重在不同节点之间容易比较，我们使用 Softmax
函数对所有的选择进行标准化： <span class="math display">\[
\alpha_{i j}=\operatorname{softmax}_{j}( e_{i j} )=\frac{exp (e_{i j}
)}{\sum \limits _{k \in \mathcal{N}_{i}} exp (e_{i k} )}
\]</span> 其中，$ ^{2 F^{}} $ 是单层前馈神经网络 <span
class="math inline">\(a\)</span> 的一个权重向量参数。<span
class="math inline">\(a\)</span> 使用 LeakyReLU
非线性激活函数（在负区间，斜率 <span
class="math inline">\(\alpha=0.2\)</span>）。</p>
<p>注意力机制的完全形式为： <span class="math display">\[
\alpha_{i j}=\frac{exp
(\operatorname{LeakyReLU}(\overrightarrow{\mathbf{a}}^{T}[\mathbf{W}
\vec{h}_{i} || \mathbf{W} \vec{h}_{j} ] ) )}{\sum \limits _{k \in
\mathcal{N}_{i}} exp
(\operatorname{LeakyReLU}(\overrightarrow{\mathbf{a}}^{T}[\mathbf{W}
\vec{h}_{i} || \mathbf{W} \vec{h}_{k} ] ) )}
\]</span> 其中， <span class="math inline">\(\vec{a}^{T} \in
\mathbb{R}^{2 F^{\prime}}\)</span> 为前馈神经网络 <span
class="math inline">\(a\)</span> 的参数，${.}^{T} $
代表着是转置操作，<span class="math inline">\(||\)</span> 是串联操作（
t.cat ）。这里最终 <span class="math inline">\(\alpha_{ij} \epsilon
{\mathbb{R}}\)</span></p>
<h4 id="通过邻居节点更新自身节点">通过邻居节点更新自身节点</h4>
<h6 id="加权求和">1. 加权求和</h6>
<p><span class="math display">\[
\vec{h}_{i}^{\prime}=\sigma(\sum \limits _{j \in
\mathcal{N}_{\mathbf{1}}} \alpha_{i j} \mathbf{W} \vec{h}_{j} )
\]</span></p>
<h6 id="multi-head-attention">2. <strong>Multi-head
Attention</strong></h6>
<p>为提高模型的拟合能力，本文引入了 multi-head attention
，即同时使用多个 <span class="math inline">\(W^{k}\)</span> 计算
self-attention，然后将各个 <span class="math inline">\(W^{k}\)</span>
计算得到的结果合并（连接concat或求和）： <span class="math display">\[
\vec{h}_{i}^{\prime}=||_{k=1}^{K} \sigma(\sum \limits _{j \in
\mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k} \vec{h}_{j} )
\]</span> 其中， <span class="math inline">\(||\)</span>
表示concat，<span class="math inline">\(a_{i j}^{k}\)</span> 表示第
<span class="math inline">\(k\)</span>
个注意机制计算的归一化注意力系数。由于 <span class="math inline">\(W^{k}
\in \mathbb{R}^{F^{\prime} \times F}\)</span> , 因此这里的 <span
class="math inline">\(\vec{h}_{i}^{\prime} \in \mathbb{R}^{K
F^{\prime}}\)</span> 。</p>
<p>如果我们对网络的<strong>最终（预测）层</strong>执行 multi-head
attention ，那么连接就不再是明智的，可以采取求和的方式来得到 <span
class="math inline">\(\vec{h}_{i}^{\prime}\)</span>： <span
class="math display">\[
\vec{h}_{i}^{\prime}=\sigma(\frac{1}{K} \sum \limits _{k=1}^{K} \sum
\limits_{j \in \mathcal{N}_{i}} \alpha_{i j}^{k} \mathbf{W}^{k}
\vec{h}_{j} )
\]</span> 　　由于 <span class="math inline">\(W^{k} \in
\mathbb{R}^{F^{\prime} \times F}\)</span> ，因此这里的 <span
class="math inline">\(\vec{h}_{i}^{\prime} \in \mathbb{R}^{
F^{\prime}}\)</span> 。</p>
<h6 id="生成注意力系数及multi-head-attention-图示">3.
生成注意力系数及Multi-head Attention 图示</h6>
<figure>
<img data-src="论文解读：GAT/image-20230129153316633.png"
alt="image-20230129153316633" />
<figcaption aria-hidden="true">image-20230129153316633</figcaption>
</figure>
<p>右图中不同颜色表示不同的head，一共有三个head。</p>
<h3 id="gat相比于先前研究的优势">GAT相比于先前研究的优势</h3>
<ul>
<li>计算高效。Self-Attention层的可以跨所有边并行化，输出特征的计算可以跨所有节点并行化。</li>
<li>可解释，且更有效。</li>
<li>Inductive learning。可以泛化到未见过的节点和图，甚至completely
unseen graph</li>
<li>解决GraphSage的缺点。GraphSage不能在推理期间访问所有邻居。</li>
<li><strong>GAT可以重新表述为MoNet的一个特定实例</strong>。更具体地说，将伪坐标函数设置为u(x，y)=
f(x)||f(y)，其中f(x)表示节点x的(一潜在的MLP变换的)特征，||表示串联；权函数为wj
(u) =
softmax(MLP(u))(在节点的整个邻域内执行softmax)将使MoNet的补丁操作符patch
operator与我们的类似。尽管如此，人们应该注意到，与以前考虑的MoNet实例相比，我们的模型使用<strong>节点特征</strong>进行相似性计算，而不是<strong>节点的结构属性</strong>(这将假设提前知道图的结构)。</li>
</ul>
<h2 id="英文写作">英文写作</h2>
<h3 id="摘要">摘要</h3>
<ul>
<li><p><strong>We present</strong> graph attention networks (GATs),
<strong>novel neural network architectures</strong> <strong>that operate
on</strong> graph-structured data, <strong>leveraging</strong> masked
self-attentional layers <strong>to address the shortcomings of</strong>
<strong>prior methods based on</strong> graph convolutions or their
approximations.</p></li>
<li><p>we <strong>enable (implicitly) specifying</strong>（指定）
different weights to different nodes</p></li>
<li><p>inversion 矩阵求逆</p></li>
<li><p><strong>In this way, we address several key challenges
of</strong> spectral-based graph neural net-works
<strong>simultaneously</strong>（同时解决了xxx）, <strong>and make our
model</strong> <strong>readily applicable to</strong>
（使我们的模型易于适用于）inductive （归纳）as well as transductive
problems（转导问题）.</p></li>
<li><p><strong>Our</strong> GAT <strong>models</strong> <strong>have
achieved or matched state-of-the-art results across</strong> four
established transductive and inductive graph
<strong>benchmarks</strong>: xx，xx，and xx datasets, as well as xx
dataset.</p></li>
</ul>
<h3 id="introduction-1">INTRODUCTION</h3>
<ul>
<li><strong>There have been several attempts in the literature（文献）
to extend</strong> neural networks <strong>to deal with</strong>
arbitrarily structured graphs. Early work used xxx</li>
<li>Attention mechanisms <strong>have become almost a <em>de facto</em>
standard</strong> （已经成为事实标准）in many sequence-based tasks</li>
<li><strong>achieving or matching state-of-the-art results that
highlight the potential of</strong> attention-based models <strong>when
dealing with</strong> arbitrarily structured graphs.</li>
</ul>
<h3 id="gat-architecture">GAT ARCHITECTURE</h3>
<ul>
<li><strong>In order to obtain sufficient expressive power to transform
the input features into higher-level
features</strong>（为了获得足够的表达能力将输入特征转换为更高阶特征）,
at least one learnable linear transformation is required.
至少需要一次可学习的线性变换</li>
</ul>
<h3 id="conclusions">CONCLUSIONS</h3>
<ul>
<li><strong>We have presented</strong> graph attention networks (GATs),
<strong>novel convolution-style neural networks that operate on</strong>
graph-structured data, <strong>leveraging</strong> masked
self-attentional layers.
我们提出了图注意网络(GATs)，这是一种新型的卷积型神经网络，它利用隐藏的自我注意层masked
self-attentional layers对图结构数据进行操作。</li>
<li>The graph attentional layer <strong>utilized throughout these
networks is computationally efficient</strong> (does not require costly
matrix operations, and is parallelizable across all nodes in the graph),
<strong>allows for</strong> (implicitly) assigning different importances
to different nodes within a neighborhood while dealing with different
sized neighborhoods, <strong>and does not depend on</strong> knowing the
entire graph structure upfront—<strong>thus addressing many of the
theoretical issues</strong> <strong>with</strong> previous
spectral-based approaches.
在这些网络中使用的图注意力层计算效率很高(不需要昂贵的矩阵运算，并且可以在图中的所有节点上并行)，允许(隐式)在处理不同大小的邻域时向邻域内的不同节点分配不同的重要性，并且不依赖于预先知道整个图结构——从而用以前的基于谱的方法解决了许多理论问题。</li>
<li><strong>Our models leveraging</strong> attention <strong>have
successfully achieved or matched state-of-the-art performance across
four well-established node classification benchmarks</strong>, both
transductive and inductive (especially, with completely unseen graphs
used for testing).
我们利用注意力的模型已经在四个完善的节点分类基准上成功实现或匹配了最先进的性能，包括转导型transductive和归纳型inductive(尤其是使用完全看不见的图进行测试)。</li>
<li><strong>There are several potential improvements and extensions
to</strong> graph attention networks <strong>that could be addressed as
future work</strong>, <strong>such as overcoming the practical problems
described in subsection 2.2</strong> to be able to handle larger batch
sizes.
图注意力网络有几个潜在的改进和扩展可以作为未来的工作来解决，比如克服第2.2款中描述的实际问题，以便能够处理更大的批量。</li>
<li><strong>A particularly interesting research direction would
be</strong> <em><strong>taking advantage of</strong> the</em> attention
mechanism <strong>to perform a thorough analysis on</strong> the model
interpretability.
一个特别有趣的研究方向将是利用注意力机制对模型的可解释性进行深入分析。</li>
<li><strong>Moreover</strong>, <strong>extending the method to
perform</strong> graph classification <strong>instead of</strong> node
classification <strong>would also be <em>relevant</em></strong>
<strong>from the application perspective</strong>.
此外，从应用的角度来看，扩展该方法来执行图分类而不是节点分类也是有意义的。</li>
<li><strong>Finally, extending the model to</strong> incorporate edge
features (possibly indicating relationship among nodes) would allow us
to <strong>tackle a larger variety of problems</strong>.
最后，扩展模型以合并边缘特征(可能指示节点之间的关系)将允许我们处理更大的各种问题。</li>
</ul>
<h2 id="typora数学公式大全">Typora数学公式大全</h2>
<p>参考：https://blog.csdn.net/Lian_Ge_Blog/article/details/125319801
<span class="math display">\[
\sum_{i+1}^m
\prod\coprod\int\oint\iint\oiint.....\infty\nabla\partial\eth\forall\exists\nexists\varnothing\mho
\]</span> <span class="math display">\[
\cong\equiv\neq\sim\simeq\approx\approxeq (****)
\leq\leqq\ll\lll\geq\geqq\gg\ggg(****)\\
\supset\supseteq\notin\in\ni\subset\subseteq
\]</span> $$ {}\ ....\</p>
<p>.....\ $$</p>
]]></content>
  </entry>
</search>
